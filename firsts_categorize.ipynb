{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 의대 프로젝트 외래 초진(first) 파싱 (Categorize)\n",
    "*를 활용해서 파싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import io\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def atof(text):\n",
    "    try:\n",
    "        retval = float(text)\n",
    "    except ValueError:\n",
    "        retval = text\n",
    "    return retval\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atof(c) for c in re.split(r'[+-]?([0-9]+(?:[.][0-9]*)?|[.][0-9]+)', text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/st/n7vhj1ln7bxdtvslhvp85fw40000gn/T/ipykernel_20316/2794179446.py:1: DtypeWarning: Columns (5,6,7,10,11,12,13,15,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_firsts = pd.DataFrame(pd.read_csv('220405goutdata/entire_cases.csv'))[['케이스번호', 'dtype', '서식내용']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>케이스번호</th>\n",
       "      <th>서식내용</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Case 1</td>\n",
       "      <td>7년전 처음 attack (+) \\n\\n연간 1-2회 정도 attack있었다\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Case 10</td>\n",
       "      <td>3년 전 Rt ankle에 attack 1주정도 아팠다\\n\\n이후 한번 더 아팠다\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case 11</td>\n",
       "      <td>EGD 상 EGC 소견 보여 pre-op w/u 중이심\\n\\nunderlying d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Case 12</td>\n",
       "      <td>Rt 1st toe 어제부터 아프다\\n\\n좀 심하게 부었다\\n\\n\\n\\nRt 1st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Case 13</td>\n",
       "      <td>2021년 5월에 과음 다음날에 Lt 1st toe가 심하게 붓고 아팠었다\\n\\n정...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     케이스번호                                               서식내용\n",
       "0   Case 1  7년전 처음 attack (+) \\n\\n연간 1-2회 정도 attack있었다\\n\\n...\n",
       "1  Case 10  3년 전 Rt ankle에 attack 1주정도 아팠다\\n\\n이후 한번 더 아팠다\\...\n",
       "2  Case 11  EGD 상 EGC 소견 보여 pre-op w/u 중이심\\n\\nunderlying d...\n",
       "3  Case 12  Rt 1st toe 어제부터 아프다\\n\\n좀 심하게 부었다\\n\\n\\n\\nRt 1st...\n",
       "4  Case 13  2021년 5월에 과음 다음날에 Lt 1st toe가 심하게 붓고 아팠었다\\n\\n정..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_firsts = pd.DataFrame(pd.read_csv('220405goutdata/entire_cases.csv'))[['케이스번호', 'dtype', '서식내용']]\n",
    "condition = (df_firsts.dtype == 'firsts')\n",
    "df_firsts = df_firsts[condition]\n",
    "\n",
    "df_firsts = df_firsts.groupby('케이스번호', as_index= False)['서식내용'].apply(lambda x: '\\n'.join(x))\n",
    "test1 = df_firsts.iloc[0]['서식내용']\n",
    "test2 = df_firsts.iloc[1]['서식내용']\n",
    "test3 = df_firsts.iloc[2]['서식내용']\n",
    "test4 = df_firsts.iloc[3]['서식내용']\n",
    "\n",
    "df_firsts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_line(string):\n",
    "    newString = string.split('\\n')\n",
    "    remove = ['', '－', '＋']\n",
    "    newString = list(filter(lambda val: val.strip() not in remove, newString))\n",
    "    # newString = (',').join(newString)\n",
    "    return newString\n",
    "\n",
    "test1 = split_by_line(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아', '나', '는', '역시', '밤', '에', '일이', '잘', '되다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "text = \"아 나는 역시 밤에 일이 잘된다.\"\n",
    "\n",
    "print(okt.morphs(text, stem=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Rank 구현\n",
    "https://github.com/lovit/textrank/ 참고\n",
    "\n",
    "아래는 모두 textrank 구현본 \n",
    "-> TextRank를 사용한 핵심문장 추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def scan_vocabulary(sents, tokenize, min_count = 2):\n",
    "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
    "    counter = {w: c for w, c in counter.items() if c >= min_count}\n",
    "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
    "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
    "    return idx_to_vocab, vocab_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def dict_to_mat(d, n_rows, n_cols):\n",
    "    rows, cols, data = [], [], []\n",
    "    for (i, j), v in d.items():\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "        data.append(v)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
    "\n",
    "\n",
    "def cooccurrence(tokens, vocab_to_idx, window=2, min_cooccurence=2):\n",
    "    counter = defaultdict(int)\n",
    "    for s, token_i in enumerate(tokens):\n",
    "        vocabs = [vocab_to_idx[w] for w in token_i if w in vocab_to_idx]\n",
    "        n = len(vocabs)\n",
    "        for i, v in enumerate(vocabs):\n",
    "            if window <= 9:\n",
    "                b, e = 0, n\n",
    "            else:\n",
    "                b = max(0, i - window)\n",
    "                e = min(i + window, n)\n",
    "            for j in range(b, e):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                counter[(v, vocabs[j])] += 1\n",
    "                counter[(vocabs[j], v)] += 1\n",
    "    counter = {k:v for k, v in counter.items() if v >= min_cooccurence}\n",
    "    n_vocabs = len(vocab_to_idx)\n",
    "    return dict_to_mat(counter, n_vocabs, n_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_graph(sents, tokenize=None, min_count=2, window=2, min_cooccurrence =2):\n",
    "    idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "    tokens = [tokenize(sent) for sent in sents]\n",
    "    g = cooccurrence(tokens, vocab_to_idx, window, min_cooccurrence)\n",
    "    return g, idx_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def pagerank(x, df=0.85, max_iter=30):\n",
    "    assert 0 < df < 1\n",
    "\n",
    "    # initialize\n",
    "    A = normalize(x, axis=0, norm='l1')\n",
    "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
    "    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
    "\n",
    "    # iteration\n",
    "    for _ in range(max_iter):\n",
    "        R = df * (A * R) + bias\n",
    "\n",
    "    return R\n",
    "\n",
    "def textrank_keyword(sents, tokenize, min_count, window, min_cooccurrence, df=0.85, max_iter=30, topk=30):\n",
    "    g, idx_to_vocab = word_graph(sents, tokenize, min_count, window, min_cooccurrence)\n",
    "    R = pagerank(g, df, max_iter).reshape(-1)\n",
    "    idxs = R.argsort()[-topk:]\n",
    "    keywords = [(idx_to_vocab[idx], R[idx]) for idx in reversed(idxs)]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def sent_graph(sents, tokenize, similarity, min_count=2, min_sim=0.3):\n",
    "    _, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "\n",
    "    tokens = [[w for w in tokenize(sent) if w in vocab_to_idx] for sent in sents]\n",
    "    rows, cols, data = [], [], []\n",
    "    n_sents = len(tokens)\n",
    "    for i, tokens_i in enumerate(tokens):\n",
    "        for j, tokens_j in enumerate(tokens):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            sim = similarity(tokens_i, tokens_j)\n",
    "            if sim < min_sim:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(sim)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))\n",
    "\n",
    "def textrank_sent_sim(s1, s2):\n",
    "    n1 = len(s1)\n",
    "    n2 = len(s2)\n",
    "    if (n1 <= 1) or (n2 <= 1):\n",
    "        return 0\n",
    "    common = len(set(s1).intersection(set(s2)))\n",
    "    base = math.log(n1) + math.log(n2)\n",
    "    return common / base\n",
    "\n",
    "def cosine_sent_sim(s1, s2):\n",
    "    if (not s1) or (not s2):\n",
    "        return 0\n",
    "\n",
    "    s1 = Counter(s1)\n",
    "    s2 = Counter(s2)\n",
    "    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))\n",
    "    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))\n",
    "    prod = 0\n",
    "    for k, v in s1.items():\n",
    "        prod += v * s2.get(k, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordSummarizer:\n",
    "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
    "        window=-1, min_cooccurrence=2, vocab_to_idx=None,\n",
    "        df=0.85, max_iter=30, verbose=False):\n",
    "\n",
    "        self.tokenize = tokenize\n",
    "        self.min_count = min_count\n",
    "        self.window = window\n",
    "        self.min_cooccurrence = min_cooccurrence\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "        self.df = df\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if sents is not None:\n",
    "            self.train_textrank(sents)\n",
    "\n",
    "    def train_textrank(self, sents, bias=None):\n",
    "        g, self.idx_to_vocab = word_graph(sents,\n",
    "            self.tokenize, self.min_count,self.window,\n",
    "            self.min_cooccurrence, self.vocab_to_idx, self.verbose)\n",
    "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
    "        if self.verbose:\n",
    "            print('trained TextRank. n words = {}'.format(self.R.shape[0]))\n",
    "\n",
    "    def keywords(self, topk=30):\n",
    "        if not hasattr(self, 'R'):\n",
    "            raise RuntimeError('Train textrank first or use summarize function')\n",
    "        idxs = self.R.argsort()[-topk:]\n",
    "        keywords = [(self.idx_to_vocab[idx], self.R[idx]) for idx in reversed(idxs)]\n",
    "        return keywords\n",
    "\n",
    "    def summarize(self, sents, topk=30):\n",
    "        self.train_textrank(sents)\n",
    "        return self.keywords(topk)\n",
    "\n",
    "\n",
    "class KeysentenceSummarizer:\n",
    "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
    "        min_sim=0.3, similarity=None, vocab_to_idx=None,\n",
    "        df=0.85, max_iter=30, verbose=False):\n",
    "\n",
    "        self.tokenize = tokenize\n",
    "        self.min_count = min_count\n",
    "        self.min_sim = min_sim\n",
    "        self.similarity = similarity\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "        self.df = df\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if sents is not None:\n",
    "            self.train_textrank(sents)\n",
    "\n",
    "    def train_textrank(self, sents, bias=None):\n",
    "        g = sent_graph(sents, self.tokenize, self.min_count,\n",
    "            self.min_sim, self.similarity, self.vocab_to_idx, self.verbose)\n",
    "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
    "        if self.verbose:\n",
    "            print('trained TextRank. n sentences = {}'.format(self.R.shape[0]))\n",
    "\n",
    "    def summarize(self, sents, topk=30, bias=None):\n",
    "        n_sents = len(sents)\n",
    "        if isinstance(bias, np.ndarray):\n",
    "            if bias.shape != (n_sents,):\n",
    "                raise ValueError('The shape of bias must be (n_sents,) but {}'.format(bias.shape))\n",
    "        elif bias is not None:\n",
    "            raise ValueError('The type of bias must be None or numpy.ndarray but the type is {}'.format(type(bias)))\n",
    "\n",
    "        self.train_textrank(sents, bias)\n",
    "        idxs = self.R.argsort()[-topk:]\n",
    "        keysents = [(idx, self.R[idx], sents[idx]) for idx in reversed(idxs)]\n",
    "        return keysents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3628689491b91afa8ba6f0b746c3e575a8d0f08e1959ddb335f40c1486723352"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

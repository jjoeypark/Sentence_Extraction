{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 의대 프로젝트 외래 경과() 파싱 (하이라이트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import io\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def atof(text):\n",
    "    try:\n",
    "        retval = float(text)\n",
    "    except ValueError:\n",
    "        retval = text\n",
    "    return retval\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atof(c) for c in re.split(r'[+-]?([0-9]+(?:[.][0-9]*)?|[.][0-9]+)', text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/st/n7vhj1ln7bxdtvslhvp85fw40000gn/T/ipykernel_43729/2029699529.py:1: DtypeWarning: Columns (5,6,7,10,11,12,13,15,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_afters = pd.DataFrame(pd.read_csv('220405goutdata/entire_cases.csv'))[['케이스번호', 'dtype', '서식내용']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>케이스번호</th>\n",
       "      <th>서식내용</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Case 1</td>\n",
       "      <td>2021.2.15 초진 -&gt; colc, feburic 처방. statin + nor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Case 10</td>\n",
       "      <td>rt ankle 많이 가라앉았다\\n\\nvimovo 좀 드심\\n\\n\\n\\n\\nurin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case 2</td>\n",
       "      <td>attack (-)\\n\\n\\n\\n\\nurinon 유지\\n\\n6달후\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Case 21</td>\n",
       "      <td>2019.4.2 초진\\n\\nfeburic 다시 복용하면서 검사하심\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Case 22</td>\n",
       "      <td>attack (-)\\n\\n6월에 요로결석으로 시술해서 뺐다\\n\\n\\n\\n\\nfebu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     케이스번호                                               서식내용\n",
       "0   Case 1  2021.2.15 초진 -> colc, feburic 처방. statin + nor...\n",
       "1  Case 10  rt ankle 많이 가라앉았다\\n\\nvimovo 좀 드심\\n\\n\\n\\n\\nurin...\n",
       "2   Case 2  attack (-)\\n\\n\\n\\n\\nurinon 유지\\n\\n6달후\\n\\n\\n\\n\\n...\n",
       "3  Case 21  2019.4.2 초진\\n\\nfeburic 다시 복용하면서 검사하심\\n\\n\\n\\n\\n...\n",
       "4  Case 22  attack (-)\\n\\n6월에 요로결석으로 시술해서 뺐다\\n\\n\\n\\n\\nfebu..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_afters = pd.DataFrame(pd.read_csv('220405goutdata/entire_cases.csv'))[['케이스번호', 'dtype', '서식내용']]\n",
    "condition = (df_afters.dtype == 'afters')\n",
    "df_afters = df_afters[condition]\n",
    "\n",
    "df_afters = df_afters.groupby('케이스번호', as_index= False)['서식내용'].apply(lambda x: '\\n'.join(x))\n",
    "test1 = df_afters.iloc[0]['서식내용']\n",
    "test2 = df_afters.iloc[1]['서식내용']\n",
    "test3 = df_afters.iloc[2]['서식내용']\n",
    "test4 = df_afters.iloc[3]['서식내용']\n",
    "\n",
    "df_afters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['2021.2.15 초진 -> colc, feburic 처방. statin + norvasc는 있으심',\n",
       "  'Gout',\n",
       "  'HTN with hyperlipidemia on med',\n",
       "  '(2/15)',\n",
       "  'ESR 8',\n",
       "  'Uric acid 8.0',\n",
       "  'AST(GOT) 28',\n",
       "  'ALT(GPT) 44',\n",
       "  'Creatinine 1.00',\n",
       "  'hs-CRP 0.07',\n",
       "  '2021-02-23',\n",
       "  '검사명 : Foot Both AP',\n",
       "  '[Conclusion]',\n",
       "  'No significant bony abnormality',\n",
       "  '약 잘 드심',\n",
       "  'attack (-)',\n",
       "  '약 다 먹었다',\n",
       "  '2달후',\n",
       "  'gout',\n",
       "  'HTN with hyperlipidemia on med',\n",
       "  'feburic, colc 유지',\n",
       "  'attack (-)',\n",
       "  '잘 지내심',\n",
       "  '고혈압약을 최근 잠깐 바궈먹었다',\n",
       "  '약 일단 유지',\n",
       "  '1달후',\n",
       "  'gout',\n",
       "  'HTN with hyperlipidemia on med',\n",
       "  'Uric acid 3.9',\n",
       "  'feburic, colc 유지',\n",
       "  'attack (-)',\n",
       "  '체중 비슷하다',\n",
       "  'feburic 잠시 hold해 보겠습니다.',\n",
       "  'colc, rosuampin 유지',\n",
       "  '6주후',\n",
       "  'gout',\n",
       "  'HTN with hyperlipidemia on med',\n",
       "  'AST(GOT) 59',\n",
       "  'ALT(GPT) 109',\n",
       "  'Uric acid 4.2',\n",
       "  'TG 278',\n",
       "  'HDL Chol. 35',\n",
       "  'LDL Chol.(계산식) 75',\n",
       "  'feburic 잠시 hold. colc, rosuampin 유지',\n",
       "  'attack (-)',\n",
       "  'colc만 유지',\n",
       "  'LFT f/u후 다시 feburic만 시작 예정',\n",
       "  '2주후',\n",
       "  'gout',\n",
       "  'HTN with hyperlipidemia on med',\n",
       "  'AST(GOT) 69',\n",
       "  'ALT(GPT) 133',\n",
       "  'BUN 16',\n",
       "  'Creatinine 1.02',\n",
       "  'Uric acid 8.4',\n",
       "  'colc, amlodipine 유지. LFT f/u 후 feburic 다시 시작 예정',\n",
       "  'UDCA 사용해 보겠습니다.',\n",
       "  'feburic 다시 시작',\n",
       "  '4주후',\n",
       "  'gout',\n",
       "  'HTN with hyperlipidemia on med',\n",
       "  'AST(GOT) 66',\n",
       "  'ALT(GPT) 125',\n",
       "  'BUN 14',\n",
       "  'Creatinine 1.01',\n",
       "  'Uric acid 9.4',\n",
       "  'LDL Chol.(계산식) 143',\n",
       "  '(8/27)',\n",
       "  'Uric acid 3.4',\n",
       "  'AST(GOT) 67',\n",
       "  'ALT(GPT) 138',\n",
       "  'Creatinine 0.97',\n",
       "  'Uric acid 3.4',\n",
       "  'feburic 다시 시작. UDCA 추가',\n",
       "  'attack (-)',\n",
       "  '오한이 한번 있었다',\n",
       "  'covid (-)',\n",
       "  'Gout',\n",
       "  'HTN with hyperlipidemia on med',\n",
       "  'LFT',\n",
       "  '1달후',\n",
       "  'feburic, colc 등 약 유지',\n",
       "  'attack (-)',\n",
       "  'herb med (-)',\n",
       "  'hold feburic',\n",
       "  'colc 유지',\n",
       "  '4주후',\n",
       "  'Gout',\n",
       "  'HTN with hyperlipidemia on med',\n",
       "  'LFT asso with feburic>>',\n",
       "  'T. Bil. 1.5',\n",
       "  'Alk. phos. 76',\n",
       "  'AST(GOT) 104',\n",
       "  'ALT(GPT) 191',\n",
       "  'BUN 19',\n",
       "  'Creatinine 1.00',\n",
       "  'Uric acid 5.1',\n",
       "  'hold feburic (LFT)',\n",
       "  'attack (-)',\n",
       "  'Gout',\n",
       "  'HTN with hyperlipidemia on med',\n",
       "  'LFT asso with feburic>>',\n",
       "  'hold feburic (LFT)',\n",
       "  'attack (-)',\n",
       "  'urinon으로 처방해 보겠습니다.',\n",
       "  '5주후 ',\n",
       "  'Gout',\n",
       "  'HTN with hyperlipidemia on med',\n",
       "  'LFT asso with feburic>>',\n",
       "  'urinon 일단 좀 더 유지',\n",
       "  'd/c colc ',\n",
       "  '나머지 약은 유지',\n",
       "  '5주후',\n",
       "  'Gout',\n",
       "  'HTN with hyperlipidemia on med',\n",
       "  'LFT asso with feburic?',\n",
       "  'Uric acid 4.2',\n",
       "  'AST(GOT) 80',\n",
       "  'ALT(GPT) 161',\n",
       "  'Creatinine 0.99',\n",
       "  'Uric acid 4.2',\n",
       "  'feburic-> uinon으로 처방해 보겠습니다. (LFT asso with feburic)',\n",
       "  'attack (-)',\n",
       "  '혈압이 좀 높았다',\n",
       "  'avodat 먹고 있다',\n",
       "  'colc 중단. urinon 등 나머지 약 유지',\n",
       "  'attack (-)',\n",
       "  'ursa 유지',\n",
       "  'urinon 유지',\n",
       "  '2달후',\n",
       "  'Gout',\n",
       "  'HTN with hyperlipidemia on med',\n",
       "  'LFT asso with feburic?',\n",
       "  'AST(GOT) 75',\n",
       "  'ALT(GPT) 140',\n",
       "  'Uric acid 4.3',\n",
       "  'urinon 유지. ursa 유지',\n",
       "  'attack (-)',\n",
       "  '체중 비슷하다',\n",
       "  'AST(GOT) 38',\n",
       "  'ALT(GPT) 71',\n",
       "  'Uric acid 4.2',\n",
       "  'Gout',\n",
       "  'HTN with hyperlipidemia on med',\n",
       "  'LFT asso with feburic? -> improving',\n",
       "  'urinon 유지',\n",
       "  'UDCA 당분간 유지',\n",
       "  '3달후'],\n",
       " 148)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_by_line(string):\n",
    "    newString = string.split('\\n')\n",
    "    remove = ['', '－', '＋']\n",
    "    newString = list(filter(lambda val: val.strip() not in remove and len(val) > 1, newString))\n",
    "    # newString = (',').join(newString)\n",
    "    return newString\n",
    "\n",
    "Ltest1 = split_by_line(test1)\n",
    "Ltest1, len(Ltest1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단순하게 생각해서 모든 문장을 나눈 이후 그 문장이 요약 소견서에 들어가있는지 안들어가 있는지를 라벨링한 데이터 프레임 생성. 이후 모델을 비교하며 (감정분석처럼) 문장의 중요도 파악. 카테고리도 비슷하게 적용시켜 볼 수 있을 것 같음.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools \n",
    "\n",
    "# sents = []\n",
    "# for i in range(len(df_firsts)):\n",
    "#     text = df_firsts.iloc[i]['서식내용']\n",
    "#     sent = split_by_line(text)\n",
    "#     sents.append(sent)\n",
    "\n",
    "# sentlist = list(itertools.chain.from_iterable(sents))\n",
    "# sents_df = pd.DataFrame({'문장': sentlist, '유무': np.nan, '카테고리': np.nan})\n",
    "\n",
    "# sents_df.to_csv('firsts_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "textList = []\n",
    "\n",
    "for i in range(len(Ltest1) - 1):\n",
    "    text = Ltest1[i]\n",
    "    parsed = okt.morphs(text, stem=True)\n",
    "    textList.append(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Rank 구현\n",
    "https://github.com/lovit/textrank/ 참고\n",
    "\n",
    "아래는 모두 textrank 구현본 \n",
    "-> TextRank를 사용한 핵심문장 추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize: okt or mecab\n",
    "\n",
    "def scan_vocabulary(sents, tokenizer):\n",
    "    counter = Counter(w for sent in sents for w in tokenizer(sent))\n",
    "    counter = {w: c for w, c in counter.items()}\n",
    "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
    "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
    "    return idx_to_vocab, vocab_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_mat(d, n_rows, n_cols):\n",
    "    rows, cols, data = [], [], []\n",
    "    for (i, j), v in d.items():\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "        data.append(v)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
    "\n",
    "\n",
    "def cooccurrence(tokens, vocab_to_idx, window=2, min_cooccurence=2):\n",
    "    counter = defaultdict(int)\n",
    "    for s, token_i in enumerate(tokens):\n",
    "        vocabs = [vocab_to_idx[w] for w in token_i if w in vocab_to_idx]\n",
    "        n = len(vocabs)\n",
    "        for i, v in enumerate(vocabs):\n",
    "            if window <= 9:\n",
    "                b, e = 0, n\n",
    "            else:\n",
    "                b = max(0, i - window)\n",
    "                e = min(i + window, n)\n",
    "            for j in range(b, e):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                counter[(v, vocabs[j])] += 1\n",
    "                counter[(vocabs[j], v)] += 1\n",
    "    counter = {k:v for k, v in counter.items() if v >= min_cooccurence}\n",
    "    n_vocabs = len(vocab_to_idx)\n",
    "    return dict_to_mat(counter, n_vocabs, n_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_graph(sents, tokenize, window, min_cooccurrence):\n",
    "    idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize)\n",
    "    tokens = [tokenize(sent) for sent in sents]\n",
    "    g = cooccurrence(tokens, vocab_to_idx, window, min_cooccurrence)\n",
    "    return g, idx_to_vocab\n",
    "\n",
    "tokenize = okt.morphs\n",
    "window = 2\n",
    "min_cooccurence = 2\n",
    "\n",
    "# word_graph(Ltest1, tokenize, window, min_cooccurence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def pagerank(x, df, max_iter):\n",
    "    assert 0 < df < 1\n",
    "\n",
    "    # initialize\n",
    "    A = normalize(x, axis=0, norm='l1')\n",
    "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
    "    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
    "\n",
    "    # iteration\n",
    "    for _ in range(max_iter):\n",
    "        R = df * (A * R) + bias\n",
    "\n",
    "    return R\n",
    "\n",
    "def textrank_keyword(sents, tokenize, window, min_cooccurrence, df, max_iter, topk):\n",
    "    g, idx_to_vocab = word_graph(sents, tokenize, window, min_cooccurrence)\n",
    "    R = pagerank(g, df, max_iter).reshape(-1)\n",
    "    idxs = R.argsort()[-topk:]\n",
    "    keywords = [(idx_to_vocab[idx], R[idx]) for idx in reversed(idxs)]\n",
    "    return keywords\n",
    "\n",
    "df = 0.70\n",
    "topk = 30\n",
    "max_iter = 30\n",
    "\n",
    "# textrank_keyword(Ltest1, tokenize, window, min_cooccurence, df, max_iter, topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def sent_graph(sents, tokenize, similarity, min_count=2, min_sim=0.3):\n",
    "    _, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "\n",
    "    tokens = [[w for w in tokenize(sent) if w in vocab_to_idx] for sent in sents]\n",
    "    rows, cols, data = [], [], []\n",
    "    n_sents = len(tokens)\n",
    "    for i, tokens_i in enumerate(tokens):\n",
    "        for j, tokens_j in enumerate(tokens):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            sim = similarity(tokens_i, tokens_j)\n",
    "            if sim < min_sim:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(sim)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))\n",
    "\n",
    "def textrank_sent_sim(s1, s2):\n",
    "    n1 = len(s1)\n",
    "    n2 = len(s2)\n",
    "    if (n1 <= 1) or (n2 <= 1):\n",
    "        return 0\n",
    "    common = len(set(s1).intersection(set(s2)))\n",
    "    base = math.log(n1) + math.log(n2)\n",
    "    return common / base\n",
    "\n",
    "def cosine_sent_sim(s1, s2):\n",
    "    if (not s1) or (not s2):\n",
    "        return 0\n",
    "\n",
    "    s1 = Counter(s1)\n",
    "    s2 = Counter(s2)\n",
    "    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))\n",
    "    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))\n",
    "    prod = 0\n",
    "    for k, v in s1.items():\n",
    "        prod += v * s2.get(k, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordSummarizer:\n",
    "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
    "        window=-1, min_cooccurrence=2, vocab_to_idx=None,\n",
    "        df=0.85, max_iter=30, verbose=False):\n",
    "\n",
    "        self.tokenize = tokenize\n",
    "        self.min_count = min_count\n",
    "        self.window = window\n",
    "        self.min_cooccurrence = min_cooccurrence\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "        self.df = df\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if sents is not None:\n",
    "            self.train_textrank(sents)\n",
    "\n",
    "    def train_textrank(self, sents, bias=None):\n",
    "        g, self.idx_to_vocab = word_graph(sents,\n",
    "            self.tokenize, self.min_count,self.window,\n",
    "            self.min_cooccurrence, self.vocab_to_idx, self.verbose)\n",
    "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
    "        if self.verbose:\n",
    "            print('trained TextRank. n words = {}'.format(self.R.shape[0]))\n",
    "\n",
    "    def keywords(self, topk=30):\n",
    "        if not hasattr(self, 'R'):\n",
    "            raise RuntimeError('Train textrank first or use summarize function')\n",
    "        idxs = self.R.argsort()[-topk:]\n",
    "        keywords = [(self.idx_to_vocab[idx], self.R[idx]) for idx in reversed(idxs)]\n",
    "        return keywords\n",
    "\n",
    "    def summarize(self, sents, topk=30):\n",
    "        self.train_textrank(sents)\n",
    "        return self.keywords(topk)\n",
    "\n",
    "\n",
    "class KeysentenceSummarizer:\n",
    "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
    "        min_sim=0.3, similarity=None, vocab_to_idx=None,\n",
    "        df=0.85, max_iter=30, verbose=False):\n",
    "\n",
    "        self.tokenize = tokenize\n",
    "        self.min_count = min_count\n",
    "        self.min_sim = min_sim\n",
    "        self.similarity = similarity\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "        self.df = df\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if sents is not None:\n",
    "            self.train_textrank(sents)\n",
    "\n",
    "    def train_textrank(self, sents, bias=None):\n",
    "        g = sent_graph(sents, self.tokenize, self.min_count,\n",
    "            self.min_sim, self.similarity, self.vocab_to_idx, self.verbose)\n",
    "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
    "        if self.verbose:\n",
    "            print('trained TextRank. n sentences = {}'.format(self.R.shape[0]))\n",
    "\n",
    "    def summarize(self, sents, topk=30, bias=None):\n",
    "        n_sents = len(sents)\n",
    "        if isinstance(bias, np.ndarray):\n",
    "            if bias.shape != (n_sents,):\n",
    "                raise ValueError('The shape of bias must be (n_sents,) but {}'.format(bias.shape))\n",
    "        elif bias is not None:\n",
    "            raise ValueError('The type of bias must be None or numpy.ndarray but the type is {}'.format(type(bias)))\n",
    "\n",
    "        self.train_textrank(sents, bias)\n",
    "        idxs = self.R.argsort()[-topk:]\n",
    "        keysents = [(idx, self.R[idx], sents[idx]) for idx in reversed(idxs)]\n",
    "        return keysents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3628689491b91afa8ba6f0b746c3e575a8d0f08e1959ddb335f40c1486723352"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
